{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MORGAN: Meta-Learning-based Few-Shot Open-Set Recognition via Generative Adversarial Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import python libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2 # 4.1.2\n",
    "import numpy as np # 1.21.5\n",
    "import matplotlib # 3.2.2\n",
    "import datetime\n",
    "import random\n",
    "\n",
    "import sklearn # 1.0.2\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import scipy # 1.4.1\n",
    "import scipy.io as sio\n",
    "\n",
    "import tensorflow as tf # 2.8.0\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.models import load_model,Model\n",
    "from tensorflow.keras import Sequential,layers,regularizers\n",
    "from tensorflow.keras.layers import Conv2D,BatchNormalization, MaxPool2D, Activation, Flatten, Dense, GlobalAveragePooling2D, GlobalMaxPool2D, AveragePooling2D, Lambda, Reshape, UpSampling2D, Conv2DTranspose \n",
    "from tensorflow.keras.layers import Conv3D,BatchNormalization, MaxPool3D, Activation, Flatten, Dense, GlobalAveragePooling3D, GlobalMaxPool3D, AveragePooling3D, Lambda\n",
    "import keras # 2.8.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load and pre-process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def applyPCA(X, numComponents=75):\n",
    "    newX = np.reshape(X, (-1, X.shape[2]))\n",
    "    pca = PCA(n_components=numComponents, whiten=True)\n",
    "    newX = pca.fit_transform(newX)\n",
    "    newX = np.reshape(newX, (X.shape[0],X.shape[1], numComponents))\n",
    "    return newX, pca\n",
    "\n",
    "def padWithZeros(X, margin=2):\n",
    "    newX = np.zeros((X.shape[0] + 2 * margin, X.shape[1] + 2* margin, X.shape[2]))\n",
    "    x_offset = margin\n",
    "    y_offset = margin\n",
    "    newX[x_offset:X.shape[0] + x_offset, y_offset:X.shape[1] + y_offset, :] = X\n",
    "    return newX\n",
    "\n",
    "def createImageCubes(X, y, windowSize, removeZeroLabels = True):\n",
    "    margin = int((windowSize - 1) / 2)\n",
    "    zeroPaddedX = padWithZeros(X, margin=margin)  # X :(145, 145, 30) --> (195, 195, 30) with window =25\n",
    "    # split patches\n",
    "    patchesData = np.zeros((X.shape[0] * X.shape[1], windowSize, windowSize, X.shape[2]))  # (21025, 25, 25, 30)   \n",
    "    patchesLabels = np.zeros((X.shape[0] * X.shape[1]))  # (21025,)\n",
    "    patchIndex = 0\n",
    "    \n",
    "    for r in range(margin, zeroPaddedX.shape[0] - margin):\n",
    "        for c in range(margin, zeroPaddedX.shape[1] - margin):\n",
    "            patch = zeroPaddedX[r - margin:r + margin + 1, c - margin:c + margin + 1]  \n",
    "            patchesData[patchIndex, :, :, :] = patch\n",
    "            patchesLabels[patchIndex] = y[r-margin, c-margin]            \n",
    "            patchIndex = patchIndex + 1\n",
    "  \n",
    "    patchesData = np.expand_dims(patchesData, axis=-1)\n",
    "    return patchesData,patchesLabels\n",
    "\n",
    "def patches_class(X,Y,n):\n",
    "    n_classes = n\n",
    "    patches_list = []\n",
    "    labeles_list = []\n",
    "    for i in range(1,n_classes+1):   # not considering class 0\n",
    "        patchesData_Ith_Label = X[Y==i,:,:,:,:]\n",
    "        Ith_Label = Y[Y==i]\n",
    "        patches_list.append(patchesData_Ith_Label)\n",
    "        labeles_list.append(Ith_Label)\n",
    "        \n",
    "    return patches_list,labeles_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Load Indian Pines (or any other Hyperspectral) Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "windowSize = 11\n",
    "im_height, im_width, im_depth, im_channel = windowSize, windowSize, 30, 1\n",
    "\n",
    "X = sio.loadmat('/content/drive/MyDrive/data/Indian_pines_corrected.mat')['indian_pines_corrected']\n",
    "y = sio.loadmat('/content/drive/MyDrive/data/Indian_pines_gt.mat')['indian_pines_gt']\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X,pca = applyPCA(X,numComponents=im_depth)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "X, y = createImageCubes(X, y, windowSize)\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "patches_class_ip,label_ip = patches_class(X,y,16) # class_wise list of patches #(16,) for class 0: (2009, 9, 9, 20, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Meta-training and Meta-testing classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class_indices = [1,2,4,5,7,9,10,11,13,14]    # 10 classes  \n",
    "train_class_labels = [2,3,5,6,8,10,11,12,14,15]\n",
    "\n",
    "test_class_indices = [0,3,6,8,12,15]               # 6 classes\n",
    "test_class_labels = [1,4,7,9,13,16]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A. Define Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1.1 CBAM3D layer for feature extractor  \n",
    "Source: \"Few-Shot Open-Set Recognition of Hyperspectral Images with Outlier Calibration Network\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Channel_Attention_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self,C,ratio) :\n",
    "        super(Channel_Attention_3D,self).__init__()\n",
    "        self.avg_pool = GlobalAveragePooling3D()\n",
    "        self.max_pool = GlobalMaxPool3D()\n",
    "        self.activation = Activation('sigmoid')\n",
    "        self.fc1 = Dense(C/ratio, activation = 'relu')\n",
    "        self.fc2 = Dense(C)\n",
    "    def call(self,x) :\n",
    "        avg_out1 = self.avg_pool(x)\n",
    "        avg_out2 = self.fc1(avg_out1)\n",
    "        avg_out3 = self.fc2(avg_out2)\n",
    "        max_out1 = self.max_pool(x)\n",
    "        max_out2 = self.fc1(max_out1)\n",
    "        max_out3 = self.fc2(max_out2)\n",
    "        add_out = tf.math.add(max_out3,avg_out3)\n",
    "        channel_att = self.activation(add_out)\n",
    "        return channel_att\n",
    "\n",
    "class Spatial_Attention_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self) :\n",
    "        super(Spatial_Attention_3D,self).__init__()\n",
    "        self.conv3d = Conv3D(1,(7,7,7),padding='same',activation='sigmoid')\n",
    "        self.avg_pool_chl = Lambda(lambda x:tf.keras.backend.mean(x,axis=4,keepdims=True))\n",
    "        self.max_pool_chl = Lambda(lambda x:tf.keras.backend.max(x,axis=4,keepdims=True)) \n",
    "        \n",
    "    def call(self,x) :\n",
    "        avg_out1 = self.avg_pool_chl(x)\n",
    "        max_out1 = self.max_pool_chl(x)\n",
    "        concat_out = tf.concat([avg_out1,max_out1],axis=-1)\n",
    "        spatial_att = self.conv3d(concat_out)\n",
    "        return spatial_att\n",
    "\n",
    "class CBAM_3D(tf.keras.layers.Layer) :\n",
    "    def __init__(self,C,ratio) :\n",
    "        super(CBAM_3D,self).__init__()\n",
    "        self.C = C\n",
    "        self.ratio = ratio\n",
    "        self.channel_attention = Channel_Attention_3D(self.C,self.ratio)\n",
    "        self.spatial_attention = Spatial_Attention_3D()\n",
    "    def call(self,y,H,W,D,C) :\n",
    "        ch_out1 = self.channel_attention(y)\n",
    "        ch_out2 = tf.expand_dims(ch_out1, axis=1)\n",
    "        ch_out3 = tf.expand_dims(ch_out2, axis=2)\n",
    "        ch_out4 = tf.expand_dims(ch_out3, axis=3)\n",
    "        ch_out5 = tf.tile(ch_out4, multiples=[1,H,W,D,1])\n",
    "        ch_out5 = tf.math.multiply(ch_out5,y)\n",
    "        sp_out1 = self.spatial_attention(ch_out5)\n",
    "        sp_out2 = tf.tile(sp_out1, multiples = [1,1,1,1,C])\n",
    "        sp_out3 = tf.math.multiply(sp_out2,ch_out5)\n",
    "        return sp_out3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.1 Feature Extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_layer = layers.Input(shape = (im_height, im_width, im_depth, im_channel))\n",
    "out1 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(input_layer)\n",
    "out2 = CBAM_3D(out1.shape[4],4)(out1,out1.shape[1],out1.shape[2],out1.shape[3],out1.shape[4])\n",
    "out2 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out1)\n",
    "out2 = CBAM_3D(out2.shape[4],4)(out2,out2.shape[1],out2.shape[2],out2.shape[3],out2.shape[4])\n",
    "out3 = layers.Conv3D(filters=8, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out2)\n",
    "out4 = layers.Add()([out1, out3])  #Concatenate()\n",
    "out5 = layers.MaxPool3D(pool_size=(2, 2, 4), strides=None, padding='same')(out4)\n",
    "\n",
    "out6 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out5)\n",
    "out6 = CBAM_3D(out6.shape[4],4)(out6,out6.shape[1],out6.shape[2],out6.shape[3],out6.shape[4])\n",
    "out7 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out6)\n",
    "out7 = CBAM_3D(out7.shape[4],4)(out7,out7.shape[1],out7.shape[2],out7.shape[3],out7.shape[4])\n",
    "out8 = layers.Conv3D(filters=16, kernel_size=(3,3,3), padding='same',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out7)\n",
    "out9 = layers.Add()([out6, out8])  #Concatenate()\n",
    "out10 = layers.MaxPool3D(pool_size=(2, 2, 2), strides=None, padding='same')(out9)\n",
    "out10 = CBAM_3D(out10.shape[4],4)(out10,out10.shape[1],out10.shape[2],out10.shape[3],out10.shape[4])\n",
    "out11 = layers.Conv3D(filters=32, kernel_size=(3,3,3), padding='valid',activation='relu',input_shape=(im_height, im_width, im_depth, im_channel))(out10)\n",
    "out12 = layers.Flatten()(out11)\n",
    "FE_model = Model(inputs=input_layer,outputs=out12,name='R3CBAM')\n",
    "FE_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.2 Outlier detector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_Feature = layers.Input(shape = (3,))\n",
    "encoded_L1 = layers.Dense(16, activation='relu')(input_Feature)\n",
    "encoded_L2 = layers.Dense(8, activation='relu')(encoded_L1)\n",
    "decoded = layers.Dense(2, activation='softmax')(encoded_L2)\n",
    "outlier_nn = Model(inputs=input_Feature,outputs=decoded,name='ONN')\n",
    "outlier_nn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.3 GAN1 : to generate pseudo-known samples: Input low noise variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.1 Generator (Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_size=16+8\n",
    "input_feature=layers.Input(shape=(generator_input_size,))\n",
    "layer_low_g1=layers.Dense(32,activation='relu')(input_feature) \n",
    "layer_low_g2=layers.Dense(48,activation='relu')(layer_low_g1)\n",
    "layer_low_g3=layers.Dense(64,activation='relu')(layer_low_g2)\n",
    "generator_nn_low=Model(inputs=input_feature,outputs=layer_low_g3, name='generator_low')\n",
    "generator_nn_low.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.3.2 Discriminator (Low)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature=layers.Input(shape=(80,))\n",
    "layer_l_d1=layers.Dense(48,activation='relu')(input_feature) \n",
    "layer_l_d2=layers.Dense(32,activation='relu')(layer_l_d1)\n",
    "layer_l_d3=layers.Dense(16,activation='relu')(layer_l_d2)\n",
    "layer_l_d4=layers.Dense(8,activation='relu')(layer_l_d3)\n",
    "layer_l_d5=layers.Dense(1)(layer_l_d4)\n",
    "dis_nn_low=Model(inputs=input_feature,outputs=layer_l_d5, name='discriminator_low')\n",
    "dis_nn_low.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.3.3 Function to optimize GAN 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_reptile(sembed,ep_class_labels,generator,dis,optim_d,optim_g,stdev,alpha1):\n",
    "    batch_size=sembed.shape[0]\n",
    "    one_hot_labels=tf.one_hot(ep_class_labels, depth=16, axis=-1)\n",
    "    one_hot_labels=tf.reshape(one_hot_labels,(batch_size,16))\n",
    "    \n",
    "    vector=tf.random.normal(shape=(batch_size,8), stddev=stdev)\n",
    "    latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "    \n",
    "    # loss_function\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # generators_low\n",
    "    generated_emb=generator(latent)\n",
    "    \n",
    "    fake_embs=tf.concat([generated_emb, one_hot_labels], axis=1)  \n",
    "    real_embs=tf.concat([sembed, one_hot_labels], axis=1)\n",
    "    combined_embs=tf.concat([fake_embs, real_embs], axis=0)\n",
    "    labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)  \n",
    "    \n",
    "    # First-order meta-Learning - Reptile\n",
    "    # Discriminator update\n",
    "    old_vars_gen=generator.get_weights()\n",
    "    old_vars_dis=dis.get_weights()\n",
    "    with tf.GradientTape() as dis_tape:\n",
    "        predictions=dis(combined_embs)\n",
    "        d_loss=loss_fn(labels,predictions)\n",
    "    grads=dis_tape.gradient(d_loss,dis.trainable_variables)\n",
    "    optim_d.apply_gradients(zip(grads, dis.trainable_variables))\n",
    "    new_vars_dis=dis.get_weights()\n",
    "    \n",
    "    for var in range(len(new_vars_dis)):\n",
    "        new_vars_dis[var]=old_vars_dis[var] + ((new_vars_dis[var]-old_vars_dis[var])*alpha1)\n",
    "    dis.set_weights(new_vars_dis) \n",
    "    \n",
    "    # Generator\n",
    "    mis_labels=tf.zeros((batch_size,1))\n",
    "    old_gen=generator.get_weights()\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        vector=tf.random.normal(shape=(batch_size,8), stddev=0.2)\n",
    "        latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "        fake_emb=generator(latent)\n",
    "        fake_emb_and_labels=tf.concat([fake_emb, one_hot_labels], axis=-1)\n",
    "        predictions=dis(fake_emb_and_labels)\n",
    "        g_loss=loss_fn(mis_labels,predictions)\n",
    "    g_grads=gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    optim_g.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    new_gen=generator.get_weights()\n",
    "    \n",
    "    for var in range(len(new_gen)):\n",
    "        new_gen[var]=old_gen[var] + ((new_gen[var]-old_gen[var])* alpha1)\n",
    "    generator.set_weights(new_gen)  \n",
    "    \n",
    "    vector=tf.random.normal(shape=(batch_size,8), stddev=stdev)\n",
    "    latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "    sembed_gen=generator(latent)\n",
    "    return sembed_gen,vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.4 GAN2 : to generate pseudo-unknown samples: Input high noise variance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.4.1 Generator (High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input_size=16+8\n",
    "input_feature=layers.Input(shape=(generator_input_size,))\n",
    "layer_h_g1=layers.Dense(32,activation='relu')(input_feature) \n",
    "layer_h_g2=layers.Dense(48,activation='relu')(layer_h_g1)\n",
    "layer_h_g3=layers.Dense(64,activation='relu')(layer_h_g2)\n",
    "generator_nn_high=Model(inputs=input_feature,outputs=layer_h_g3, name='generator_high')\n",
    "generator_nn_high.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.4.2 Discriminator (High)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature=layers.Input(shape=(80,))\n",
    "layer_h_d1=layers.Dense(48,activation='relu')(input_feature) \n",
    "layer_h_d2=layers.Dense(32,activation='relu')(layer_h_d1)\n",
    "layer_h_d3=layers.Dense(16,activation='relu')(layer_h_d2)\n",
    "layer_h_d4=layers.Dense(8,activation='relu')(layer_h_d3)\n",
    "layer_h_d5=layers.Dense(1)(layer_h_d4)\n",
    "dis_nn_high=Model(inputs=input_feature,outputs=layer_h_d5, name='discriminator_high')\n",
    "dis_nn_high.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### A.4.3 AOL Regularizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_loss(s1,s2,z1,z2):\n",
    "    s1=tf.nn.l2_normalize(s1,dim=1)\n",
    "    s2=tf.nn.l2_normalize(s2,dim=1)\n",
    "    z1=tf.nn.l2_normalize(z1,dim=1)\n",
    "    z2=tf.nn.l2_normalize(z2,dim=1)\n",
    "    cos_s=s1*s2\n",
    "    cos_z=z1*z2\n",
    "    loss = (1+tf.reduce_sum(cos_z,axis=1))*(tf.math.maximum(0.0000001, tf.reduce_sum(cos_s,axis=1))) \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A.4.4 Function to optimize GAN 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gan_reptile_high(sembed,sembed_low,z1,ep_class_labels,generator,dis,optim_d,optim_g,stdev,alpha1):\n",
    "    batch_size=sembed.shape[0]\n",
    "    one_hot_labels=tf.one_hot(ep_class_labels, depth=16, axis=-1)\n",
    "    one_hot_labels=tf.reshape(one_hot_labels,(batch_size,16))\n",
    "    \n",
    "    vector=tf.random.normal(shape=(batch_size,8), stddev=stdev)\n",
    "    latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "    \n",
    "    # loss_function\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "    \n",
    "    # generators_High\n",
    "    generated_emb=generator(latent)\n",
    "    # AOL regulazer loss\n",
    "    c_lossd=cosine_loss(generated_emb,sembed_low,vector,z1)\n",
    "    \n",
    "    fake_embs=tf.concat([generated_emb, one_hot_labels], axis=1)  \n",
    "    real_embs=tf.concat([sembed, one_hot_labels], axis=1)\n",
    "    combined_embs=tf.concat([fake_embs, real_embs], axis=0)  \n",
    "    labels = tf.concat([tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0)  \n",
    "    \n",
    "    # First-order meta-Learning - Reptile \n",
    "    # discriminator\n",
    "    old_vars_gen=generator.get_weights()\n",
    "    old_vars_dis=dis.get_weights()\n",
    "    with tf.GradientTape() as dis_tape:\n",
    "        predictions=dis(combined_embs)\n",
    "        d_loss=loss_fn(labels,predictions) + c_lossd\n",
    "    grads=dis_tape.gradient(d_loss,dis.trainable_variables)\n",
    "    optim_d.apply_gradients(zip(grads, dis.trainable_variables))\n",
    "    new_vars_dis=dis.get_weights()\n",
    "    \n",
    "    for var in range(len(new_vars_dis)):\n",
    "        new_vars_dis[var]=old_vars_dis[var] + ((new_vars_dis[var]-old_vars_dis[var])*alpha1)\n",
    "    dis.set_weights(new_vars_dis) \n",
    "    \n",
    "    # generator\n",
    "    mis_labels=tf.zeros((batch_size,1))\n",
    "    old_gen=generator.get_weights()\n",
    "    with tf.GradientTape() as gen_tape:\n",
    "        vector=tf.random.normal(shape=(batch_size,8), stddev=0.2)\n",
    "        latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "        fake_emb=generator(latent)\n",
    "        c_lossg=cosine_loss(fake_emb,sembed_low,vector,z1)\n",
    "        fake_emb_and_labels=tf.concat([fake_emb, one_hot_labels], axis=-1)\n",
    "        predictions=dis(fake_emb_and_labels)\n",
    "        g_loss=loss_fn(mis_labels,predictions) + c_lossg\n",
    "    g_grads=gen_tape.gradient(g_loss, generator.trainable_variables)\n",
    "    optim_g.apply_gradients(zip(g_grads, generator.trainable_variables))\n",
    "    new_gen=generator.get_weights()\n",
    "    for var in range(len(new_gen)):\n",
    "        new_gen[var]=old_gen[var] + ((new_gen[var]-old_gen[var])* alpha1)\n",
    "    generator.set_weights(new_gen)  \n",
    "    \n",
    "    vector=tf.random.normal(shape=(batch_size,8), stddev=stdev)\n",
    "    latent=tf.concat([vector,one_hot_labels], axis=1)\n",
    "    sembed_gen=generator(latent)\n",
    "    return sembed_gen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B. Define Optimizer and Distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bce = tf.keras.losses.BinaryCrossentropy(from_logits=True)\n",
    "optim1 = tf.keras.optimizers.Adam(0.0001) \n",
    "optim2 = tf.keras.optimizers.Adam(0.0001) \n",
    "optim_d_low=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "optim_g_low=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "optim_d_high=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "optim_g_high=tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
    "scce = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "ce_loss = tf.keras.losses.CategoricalCrossentropy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints to save meta-training updates\n",
    "checkpoint_dir = '/content/Rp'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optim1=optim1, optim2=optim2, optim_d_low=optim_d_low, optim_g_low=optim_g_low, optim_d_high=optim_d_high,\n",
    "                                 optim_g_high=optim_g_high, FE_model = FE_model,\n",
    "                                 generator_nn_low=generator_nn_low, generator_nn_high=generator_nn_high,\n",
    "                                 dis_nn_low=dis_nn_low, dis_nn_high=dis_nn_high,\n",
    "                                 outlier_nn=outlier_nn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Distance functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_euclidian_dists(x, y):\n",
    "  # x : (n,d)\n",
    "  # y : (m,d)\n",
    "    n = x.shape[0]\n",
    "    m = y.shape[0]\n",
    "    x = tf.tile(tf.expand_dims(x, 1), [1, m, 1])\n",
    "    y = tf.tile(tf.expand_dims(y, 0), [n, 1, 1])\n",
    "    return tf.reduce_mean(tf.math.pow(x - y, 2), 2)\n",
    "\n",
    "def rp_distance(embed,rp,num_rec_points=1,num_class=3):   \n",
    "    # Reciprocal points : https://arxiv.org/pdf/2103.00953.pdf\n",
    "    f_2=tf.reduce_sum(tf.math.pow(embed,2), axis=1, keepdims=True)\n",
    "    c_2=tf.reduce_sum(tf.math.pow(rp,2), axis=1, keepdims=True)\n",
    "    dist=f_2 - 2*tf.linalg.matmul(tf.cast(embed,tf.float64),tf.transpose(rp)) + tf.transpose(c_2)\n",
    "    dist=dist/float(embed.shape[1])\n",
    "    dist=tf.reshape(dist, [-1, num_class, num_rec_points])\n",
    "    dist=tf.reduce_mean(dist, axis=2)\n",
    "    return dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 64"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## C. Meta-training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode for Meta-training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def new_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_train(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N): #5,3,6,15     \n",
    "    outlier=0\n",
    "    sembed=FE_model(ep_class_images)\n",
    "    qembed=FE_model(ep_query_images)\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)\n",
    "    y_true = np.zeros(len(ep_query_labels))\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "        if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            \n",
    "    # gan --> reptile\n",
    "    ngan = 5 \n",
    "    for _ in range(ngan):\n",
    "        sembed_low,vector=gan_reptile(sembed,ep_class_labels,generator_nn_low,dis_nn_low,optim_d_low,optim_g_low,stdev=0.2,alpha1=0.003)\n",
    "        sembed_high=gan_reptile_high(sembed,sembed_low,vector,ep_class_labels,generator_nn_high,dis_nn_high,optim_d_high,optim_g_high,stdev=1.0,alpha1=0.003)\n",
    "        \n",
    "    # Reciprocal prototypes\n",
    "    z_proto_low = tf.reshape(sembed_low,[CS, K, sembed_low.shape[-1]])      # [3, 5, 64]\n",
    "    z_prototypes = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean((z_prototypes + z_proto_low), axis=1) # [3, 5, 64]\n",
    "    \n",
    "    rp_proto_low = tf.reshape(sembed,[CS, K, sembed.shape[-1]])\n",
    "    rp_proto_low = tf.reduce_mean(rp_proto_low, axis=1)\n",
    "    \n",
    "    rp_proto_high = tf.reshape(qembed, [CQ, N, qembed.shape[-1]])\n",
    "    rp_proto_high = tf.reduce_mean(rp_proto_high, axis=1)\n",
    "    rp_proto_high = tf.reduce_mean(rp_proto_high, axis=0, keepdims=True)\n",
    "    \n",
    "    rpoints = tf.concat([rp_proto_low,rp_proto_high], 0)\n",
    "    \n",
    "    sqembedK = np.zeros((CS*(N+K),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(N+K),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "        if np.sum(y_query[i,:])==1:     # k query \n",
    "            y_sqK[j,:] = y_query[i,:]\n",
    "            sqembedK[j,:] = qembed[i,:]   # [45,3]\n",
    "            j = j + 1\n",
    "            \n",
    "    for i in range(len(sembed)) :\n",
    "        sqembedK[j,:] = sembed[i,:]\n",
    "        y_sqK[j,:] = y_support[i,:]\n",
    "        j = j + 1\n",
    "        \n",
    "    # compute the reciprocal loss and train the reciprocal points\n",
    "    sqembedKU = tf.concat([sembed, qembed], 0) \n",
    "    y_sqku = np.concatenate((y_support, y_query), 0)\n",
    "    y_sqKU = np.zeros((y_sqku.shape[0],4))\n",
    "    y_sqKU[:,1:4]=y_sqku\n",
    "    dist_rp = rp_distance(tf.cast(sqembedKU,dtype=tf.float64) ,tf.cast(rpoints,tf.float64),1,4)     # [105,64] , rp-->[4*10,64] , dist_rp-->[105,4]\n",
    "    dist_rp=tf.nn.log_softmax(dist_rp) # dist_rp-->[60,3]\n",
    "    loss_rp=tf.multiply(tf.convert_to_tensor(y_sqKU, dtype=tf.float32),tf.cast(dist_rp,tf.float32)) # y_sqKU--->[60,4]\n",
    "    loss_rp = -tf.reduce_mean(loss_rp, axis=-1)\n",
    "    \n",
    "    y_sq_Aug = y_sqK            # (Q+S)K + (S)K(Gen) + QU\n",
    "    sqembed_Aug = sqembedK              # query + support knowns\n",
    "    sqembed_Aug = tf.concat((sqembed_Aug,sembed_high),axis=0)  # stacking genrated known [k class]>AE o/p \n",
    "    y_sq_Aug = tf.concat((y_sq_Aug,y_support),axis=0)\n",
    "    for i in range(len(y_query)):     # 90\n",
    "        if np.sum(y_query[i,:])==0:     # u query \n",
    "            y_sq_Aug = tf.concat((y_sq_Aug,tf.expand_dims(y_query[i,:],axis=0)),axis=0)\n",
    "            sqembed_Aug = tf.concat((sqembed_Aug,tf.expand_dims(qembed[i,:],axis=0)),axis=0)\n",
    "    dists_Aug = calc_euclidian_dists(sqembed_Aug, z_prototypes) \n",
    "    log_p_y_Aug = tf.nn.log_softmax(-dists_Aug,axis=-1)\n",
    "    cec_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sq_Aug, log_p_y_Aug), axis=-1))) \n",
    "    \n",
    "    #outlier detection [outlier network update]\n",
    "    with tf.GradientTape() as outlier_tape:\n",
    "        outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "        y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "        for i in range(len(y_sq_Aug)) :\n",
    "            if i < (CS*(K*2+N)):\n",
    "                y_outlier[i] = 1\n",
    "        outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "    grads = outlier_tape.gradient(outlier_loss, outlier_nn.trainable_variables)\n",
    "    optim1.apply_gradients(zip(grads, outlier_nn.trainable_variables))\n",
    "    \n",
    "    \n",
    "    # outlier loss calculation for FE update\n",
    "    outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "    y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "    for i in range(len(y_sq_Aug)) :\n",
    "        if i < (CS*(K*2+N)):\n",
    "            y_outlier[i] = 1\n",
    "    outlier_loss = 10*scce(y_outlier,outlier_pred)  \n",
    "    \n",
    "    \n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            if outlier_index[i] == 1 :\n",
    "                x = support_classes.index(ep_query_labels[i])\n",
    "                if x == pred_index[i] :\n",
    "                    correct_pred += 1  \n",
    "            else :\n",
    "                if outlier_index[i] == 0 :\n",
    "                    outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*N)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*N)\n",
    "    \n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "            y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "            y_pred[i] = 0\n",
    "            \n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "    loss = cec_loss + outlier_loss + loss_rp \n",
    "    return loss, accuracy, outlier_det_acc, open_oa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "train_loss = tf.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.metrics.Mean(name='train_accuracy')\n",
    "train_openoa = tf.metrics.Mean(name='train_openoa')\n",
    "train_outlier_acc = tf.metrics.Mean(name='train_outlier_acc')\n",
    "\n",
    "def train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, openoa = meta_train(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,K,CS,CQ,N)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim2.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    train_loss(loss)\n",
    "    train_acc(accuracy)\n",
    "    train_openoa(openoa)\n",
    "    train_outlier_acc(outlier_det_acc)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "train_log_dir = 'logs/gradient_tape/' + current_time + '/train'\n",
    "train_summary_writer = tf.summary.create_file_writer(train_log_dir)\n",
    "\n",
    "for epoch in range(5001): \n",
    "    train_loss.reset_states()  \n",
    "    train_acc.reset_states()\n",
    "    train_openoa.reset_states()\n",
    "    train_outlier_acc.reset_states()\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = new_episode(patches_class_ip,5,15,3,6,train_class_labels)   \n",
    "        train_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,5,3,6,15)   \n",
    "    with train_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', train_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', train_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('openoa',train_openoa.result(), step=epoch)\n",
    "        tf.summary.scalar('outlier_det_acc',train_outlier_acc.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Train Loss: {:.2f}, Train Accuracy: {:.2f}, Train Open OA: {:.2f}, Train Outlier Det. Acc: {:.2f}'\n",
    "    print(template.format(epoch+1,train_loss.result(),train_acc.result()*100,train_openoa.result()*100,train_outlier_acc.result()*100))\n",
    "    max_all=0.0\n",
    "    if max_all<(train_acc.result()*100+train_openoa.result()*100+train_outlier_acc.result()*100):\n",
    "        max_all=train_acc.result()*100+train_openoa.result()*100+train_outlier_acc.result()*100\n",
    "        checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## D. Meta-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_class_indices = [1,2,4,5,7,9,10,11,13,14]    # 10 classes  \n",
    "#train_class_labels = [2,3,5,6,8,10,11,12,14,15]\n",
    "#test_class_indices = [0,3,6,8,12,15]               # 6 classes\n",
    "#test_class_labels = [1,4,7,9,13,16]\n",
    "\n",
    "train_patches_class = [patches_class_ip[i] for i in train_class_indices]        #(10)\n",
    "test_patches_class = [patches_class_ip[i] for i in test_class_indices]        #(6) \n",
    " \n",
    "test_support_labels = [16,4,13]\n",
    "ft_labels = [2,3,4,5,6,8,10,11,12,13,14,15,16]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tune_set_5 = [[] for i in range(16)]\n",
    "for j in range(1,17) :\n",
    "    if j in train_class_labels :\n",
    "        tune_set_5[j-1] = patches_class_ip[j-1] \n",
    "    elif j in test_support_labels :\n",
    "        tune_set_5[j-1] = patches_class_ip[j-1][:5,:,:,:,:] # for each class first 5 samples taken"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Episode for fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tune_episode(patches_list,NS,NQ,CS,CQ,class_labels) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = list(np.random.choice(class_labels,CQ,replace=False))  # Randomly choice 6 Query Classes\n",
    "    support_classes = list(np.random.choice(selected_classes,CS,replace=False))  # Randomly choice 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = tune_episode(tune_set_5,1,4,3,6,ft_labels)\n",
    "print(tsupport_patches.shape,tquery_patches.shape,support_labels,query_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checkpoints to save meta-tuning updates\n",
    "checkpoint_dir_tune = '/content/Tune_Rp'\n",
    "checkpoint_prefix_tune = os.path.join(checkpoint_dir_tune, \"ckpt\")\n",
    "checkpoint_tune = tf.train.Checkpoint(optim1=optim1, optim2=optim2, optim_d_low=optim_d_low, optim_g_low=optim_g_low, optim_d_high=optim_d_high,\n",
    "                                 optim_g_high=optim_g_high, FE_model = FE_model,\n",
    "                                 generator_nn_low=generator_nn_low, generator_nn_high=generator_nn_high,\n",
    "                                 dis_nn_low=dis_nn_low, dis_nn_high=dis_nn_high,\n",
    "                                 outlier_nn=outlier_nn)\n",
    "\n",
    "ngan = 5 \n",
    "emb_dim = 64\n",
    "tK = 1\n",
    "tN = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_tune(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,tK,CS,CQ,tN):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "        if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.      \n",
    "    \n",
    "    for _ in range(ngan):    \n",
    "        sembed_low,vector=gan_reptile(sembed,ep_class_labels,generator_nn_low,dis_nn_low,optim_d_low,optim_g_low,stdev=0.2,alpha1=0.003)\n",
    "        sembed_high=gan_reptile_high(sembed,sembed_low,vector,ep_class_labels,generator_nn_high,dis_nn_high,optim_d_high,optim_g_high,stdev=1.0,alpha1=0.003)\n",
    "    \n",
    "    z_proto_low = tf.reshape(sembed_low,[CS, tK, sembed_low.shape[-1]])      # [3, 1, 64]\n",
    "    z_prototypes = tf.reshape(sembed,[CS, tK, sembed.shape[-1]])           # [3, 1, 64]\n",
    "    z_prototypes = tf.math.reduce_mean((z_prototypes + z_proto_low), axis=1) # [3, 1, 64]\n",
    "\n",
    "    rp_proto_low = tf.reshape(sembed,[CS, tK, sembed.shape[-1]])\n",
    "    rp_proto_low = tf.reduce_mean(rp_proto_low, axis=1)\n",
    "\n",
    "    rp_proto_high = tf.reshape(qembed, [CQ, tN, qembed.shape[-1]])\n",
    "    rp_proto_high = tf.reduce_mean(rp_proto_high, axis=1)\n",
    "    rp_proto_high = tf.reduce_mean(rp_proto_high, axis=0, keepdims=True)\n",
    "\n",
    "    rpoints = tf.concat([rp_proto_low,rp_proto_high], 0)\n",
    "    \n",
    "\n",
    "    # Vautoencoder Loss on Query + Support\n",
    "    rec_kl_loss = 0\n",
    "    clf_loss = 0\n",
    "    sqembedK = np.zeros((CS*(tN+tK),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(tN+tK),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "        if np.sum(y_query[i,:])==1:     # k query \n",
    "            y_sqK[j,:] = y_query[i,:]\n",
    "            sqembedK[j,:] = qembed[i,:]\n",
    "            j = j + 1\n",
    "    for i in range(len(sembed)) :\n",
    "        sqembedK[j,:] = sembed[i,:]\n",
    "        y_sqK[j,:] = y_support[i,:]\n",
    "        j = j + 1\n",
    "    sqembedKU = tf.concat([sembed, qembed], 0) \n",
    "    y_sqku = np.concatenate((y_support, y_query), 0)\n",
    "    y_sqKU = np.zeros((y_sqku.shape[0],4))\n",
    "    y_sqKU[:,1:4]=y_sqku\n",
    "    dist_rp = rp_distance(tf.cast(sqembedKU,dtype=tf.float64) ,tf.cast(rpoints,tf.float64),1,4)     # [105,64] , rp-->[4*10,64] , dist_rp-->[105,4]\n",
    "    dist_rp=tf.nn.log_softmax(dist_rp) # dist_rp-->[60,3]\n",
    "    loss_rp=tf.multiply(tf.convert_to_tensor(y_sqKU, dtype=tf.float32),tf.cast(dist_rp,tf.float32)) # y_sqKU--->[60,4]\n",
    "    loss_rp = -tf.reduce_mean(loss_rp, axis=-1)\n",
    "\n",
    "    \n",
    "    # Query set Augmentation((S + QK)(Original + Gen)) [CEC loss for FE]\n",
    "    y_sq_Aug = y_sqK            # (Q+S)K + (S)K(Gen) + QU\n",
    "    sqembed_Aug = sqembedK              # query + support knowns\n",
    "    sqembed_Aug = tf.concat((sqembed_Aug,sembed_high),axis=0)  # stacking genrated known [k class]>AE o/p  \n",
    "    y_sq_Aug = tf.concat((y_sq_Aug,y_support),axis=0)\n",
    "    for i in range(len(y_query)):     # 90\n",
    "        if np.sum(y_query[i,:])==0:     # u query \n",
    "            y_sq_Aug = tf.concat((y_sq_Aug,tf.expand_dims(y_query[i,:],axis=0)),axis=0)\n",
    "            sqembed_Aug = tf.concat((sqembed_Aug,tf.expand_dims(qembed[i,:],axis=0)),axis=0)\n",
    "    dists_Aug = calc_euclidian_dists(sqembed_Aug, z_prototypes) \n",
    "    log_p_y_Aug = tf.nn.log_softmax(-dists_Aug,axis=-1)\n",
    "    cec_loss = -tf.reduce_mean((tf.reduce_sum(tf.multiply(y_sq_Aug, log_p_y_Aug), axis=-1))) \n",
    "    \n",
    "    #outlier detection [outlier network update]\n",
    "    with tf.GradientTape() as outlier_tape:\n",
    "        outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "        y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "        for i in range(len(y_sq_Aug)) :\n",
    "            if i < (CS*(tK*2+tN)):\n",
    "                y_outlier[i] = 1\n",
    "        outlier_loss = 10*scce(y_outlier,outlier_pred)\n",
    "    grads = outlier_tape.gradient(outlier_loss, outlier_nn.trainable_variables)\n",
    "    optim1.apply_gradients(zip(grads, outlier_nn.trainable_variables))\n",
    "\n",
    "    # outlier loss calculation for FE update\n",
    "    outlier_pred = outlier_nn(dists_Aug) #0 for unseen, 1 for seen\n",
    "    y_outlier = np.zeros(len(y_sq_Aug)) #labels for outliers, 0 - unseen, 1 - seen\n",
    "    for i in range(len(y_sq_Aug)) :\n",
    "        if i < (CS*(tK*2+tN)):\n",
    "            y_outlier[i] = 1\n",
    "    outlier_loss = 10*scce(y_outlier,outlier_pred)  \n",
    "\n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            if outlier_index[i] == 1 :\n",
    "                x = support_classes.index(ep_query_labels[i])\n",
    "                if x == pred_index[i] :\n",
    "                    correct_pred += 1  \n",
    "        else :\n",
    "            if outlier_index[i] == 0 :\n",
    "                outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*tN)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*tN)\n",
    "\n",
    "\n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "            y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "            y_pred[i] = 0\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))\n",
    "    \n",
    "   \n",
    "    loss = cec_loss + outlier_loss + loss_rp\n",
    "    return loss, accuracy, outlier_det_acc, open_oa    # scalar, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Metrics to gather\n",
    "tune_loss = tf.metrics.Mean(name='tune_loss')\n",
    "tune_acc = tf.metrics.Mean(name='tune_accuracy')\n",
    "tune_open_acc = tf.metrics.Mean(name='tune_open_accuracy')\n",
    "tune_outlier_acc = tf.metrics.Mean(name='tune_outlier_accuracy')\n",
    "\n",
    "def tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN):\n",
    "    # Forward & update gradients\n",
    "    with tf.GradientTape() as tape:\n",
    "        loss, accuracy, outlier_det_acc, open_oa = meta_tune(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,CS,CQ,tN)\n",
    "    gradients = tape.gradient(loss, FE_model.trainable_variables)\n",
    "    optim2.apply_gradients(zip(gradients, FE_model.trainable_variables))\n",
    "    # Log loss and accuracy for step\n",
    "    tune_loss(loss)\n",
    "    tune_acc(accuracy)\n",
    "    tune_open_acc(open_oa)\n",
    "    tune_outlier_acc(outlier_det_acc)\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "tune_log_dir = 'logs/gradient_tape/' + current_time + '/tune'\n",
    "tune_summary_writer = tf.summary.create_file_writer(tune_log_dir)\n",
    "        \n",
    "for epoch in range(1001): \n",
    "    tune_loss.reset_states()  \n",
    "    tune_acc.reset_states()\n",
    "    tune_open_acc.reset_states()\n",
    "    tune_outlier_acc.reset_states()\n",
    "    for epi in range(10): \n",
    "        tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = tune_episode(tune_set_5,1,4,3,6,ft_labels)   \n",
    "        tune_step(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,tK,3,6,tN)\n",
    "    \n",
    "    with tune_summary_writer.as_default():\n",
    "        tf.summary.scalar('loss', tune_loss.result(), step=epoch)\n",
    "        tf.summary.scalar('accuracy', tune_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('Open_accuracy', tune_open_acc.result(), step=epoch)\n",
    "        tf.summary.scalar('Outlier_accuracy', tune_outlier_acc.result(), step=epoch)\n",
    "\n",
    "    template = 'Epoch {}, Tune Loss: {:.2f}, Tune Accuracy: {:.2f}, Open Accuracy: {:.2f},Outlier Accuracy: {:.2f}'\n",
    "    print(template.format(epoch+1,tune_loss.result(),tune_acc.result()*100,tune_open_acc.result()*100,tune_outlier_acc.result()*100))\n",
    "\n",
    "    max_all=0.0\n",
    "    if max_all<(tune_acc.result()*100+tune_open_acc.result()*100+tune_outlier_acc.result()*100):\n",
    "        max_all=tune_acc.result()*100+tune_open_acc.result()*100+tune_outlier_acc.result()*100\n",
    "        checkpoint_tune.save(file_prefix = checkpoint_prefix_tune)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## E. Meta-testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def meta_test(ep_class_images,ep_query_images,ep_class_labels,ep_query_labels,support_classes,K,CS,CQ,N):#5,3,6,15     \n",
    "    outlier = 0\n",
    "    sembed = FE_model(ep_class_images)                             # [15, 64]        \n",
    "    qembed = FE_model(ep_query_images)                             # [90, 64]\n",
    "    y_query = np.asarray(np.zeros((len(ep_query_images),CS)),dtype=np.float32)  # (90, 3) \n",
    "    y_true = np.zeros(len(ep_query_labels)) #for storing labels of classes, 0 for unseen; 1,2,3 for the three classes\n",
    "    y_auc = np.zeros((len(ep_query_labels))) #for storing labels, 1 for seen, and 0 for unseen\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_query_labels[i])\n",
    "            y_query[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "            y_true[i] = x+1\n",
    "            y_auc[i] = 1\n",
    "    y_support = np.asarray(np.zeros((len(ep_class_images),CS)),dtype=np.float32)  # (15, 3) \n",
    "    for i in range(len(ep_class_labels)) :\n",
    "        if ep_class_labels[i] in support_classes :\n",
    "            x = support_classes.index(ep_class_labels[i])\n",
    "            y_support[i][x] = 1.                                      # [[0., 0., 1.], [0., 0., 0.], ... (90,3)\n",
    "    \n",
    "    z_prototypes = tf.reshape(sembed,[CS, K, sembed.shape[-1]])           # [3, 5, 64]\n",
    "    z_prototypes = tf.math.reduce_mean(z_prototypes, axis=1)        # [3, 64]   \n",
    "\n",
    "    sqembedK = np.zeros((CS*(N+K),emb_dim)) #known query then support samples\n",
    "    y_sqK = np.asarray(np.zeros((CS*(N+K),CS)),dtype=np.float32) # QK + SK\n",
    "    j = 0\n",
    "    for i in range(len(y_query)):     # 90\n",
    "        if np.sum(y_query[i,:])==1:     # k query \n",
    "            y_sqK[j,:] = y_query[i,:]\n",
    "            sqembedK[j,:] = qembed[i,:]\n",
    "            j = j + 1\n",
    "    for i in range(len(sembed)) :\n",
    "        sqembedK[j,:] = sembed[i,:]\n",
    "        y_sqK[j,:] = y_support[i,:]\n",
    "        j = j + 1\n",
    "   \n",
    "    #accuracy calculation\n",
    "    correct_pred = 0\n",
    "    dists = calc_euclidian_dists(qembed, z_prototypes)               # [90, 3]  \n",
    "    outlier_prob = outlier_nn(dists)\n",
    "    outlier_prob1 = outlier_prob\n",
    "    outlier_index = tf.argmax(outlier_prob,axis=-1)\n",
    "    predictions = tf.nn.softmax(-dists, axis=-1)\n",
    "    pred = predictions\n",
    "    pred2 = predictions\n",
    "    pred_index = tf.argmax(predictions,axis=-1)\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if ep_query_labels[i] in support_classes :\n",
    "            if outlier_index[i] == 1 :\n",
    "                x = support_classes.index(ep_query_labels[i])\n",
    "                if x == pred_index[i] :\n",
    "                    correct_pred += 1  \n",
    "        else :\n",
    "            if outlier_index[i] == 0 :\n",
    "                outlier = outlier + 1  \n",
    "    accuracy = correct_pred/(CS*N)     # scalar      \n",
    "    outlier_det_acc = outlier/((CQ-CS)*N)\n",
    "    y_score = np.zeros((len(ep_query_labels)))\n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        y_score[i] = outlier_prob1[i,1]\n",
    "    auc = sklearn.metrics.roc_auc_score(y_auc, y_score)\n",
    "\n",
    "\n",
    "    #open oa\n",
    "    y_pred = np.zeros((len(ep_query_labels))) \n",
    "    for i in range(len(ep_query_labels)) :\n",
    "        if outlier_index[i] == 1 :\n",
    "            y_pred[i] = pred_index[i]+1\n",
    "        else :\n",
    "            y_pred[i] = 0\n",
    "    cm = confusion_matrix(y_true,y_pred)\n",
    "    FP = cm.sum(axis=0) - np.diag(cm)  \n",
    "    FN = cm.sum(axis=1) - np.diag(cm)\n",
    "    TP = np.diag(cm)\n",
    "    TN = cm.sum() - (FP + FN + TP)\n",
    "    open_oa = (sum(TP)+sum(TN))/(sum(TP)+sum(TN)+sum(FP)+sum(FN))    \n",
    "    return accuracy, open_oa, outlier_det_acc, auc    # scalar, scalar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_episode(patches_list,NS,NQ,CS,CQ) :  # NS 5,NQ 15,CS 3,CQ 6\n",
    "    selected_classes = test_class_labels  # 6 Query Classes\n",
    "    support_classes = test_support_labels  # 3 Support Classes from Q\n",
    "    \n",
    "    tquery_patches,tsupport_patches = [],[]\n",
    "    query_labels,support_labels = [],[]\n",
    "    \n",
    "    for x in support_classes :      #3\n",
    "        sran_indices = np.random.choice(patches_list[x-1].shape[0],NS,replace=False) # K=5 img nos from 20 per class for Support\n",
    "        support_patches = patches_list[x-1][sran_indices,:,:,:,:]  # for x class those 5 nos\n",
    "        tsupport_patches.extend(support_patches)               # 3 class * 5 patch per class = 15 patches\n",
    "        for i in range(NS) :\n",
    "            support_labels.append(x)                           # 3 class *5 nos = 15 nos\n",
    "        \n",
    "    for x in selected_classes :     #6\n",
    "        qran_indices = np.random.choice(patches_list[x-1].shape[0],NQ,replace=False) # NQ=15 img nos from 20 per class for Query\n",
    "        query_patches = patches_list[x-1][qran_indices,:,:,:,:]    # for x class those 15 nos\n",
    "        tquery_patches.extend(query_patches)                   # 6 class * 15 patch per class = 90 patches\n",
    "        for i in range(NQ) :\n",
    "            query_labels.append(x)                             # 6 class * 15 nos = 90 nos\n",
    "    \n",
    "    temp1 = list(zip(tquery_patches, query_labels)) \n",
    "    random.shuffle(temp1)        # By Doing Shuffling, Support, Query Same class combination got mismatched - mitigated by support index\n",
    "    tquery_patches, query_labels = zip(*temp1)\n",
    "    \n",
    "    tquery_patches = tf.convert_to_tensor(np.reshape(np.asarray(tquery_patches),(CQ*NQ,11,11,30,1)),dtype=tf.float32)\n",
    "    tsupport_patches = tf.convert_to_tensor(np.reshape(np.asarray(tsupport_patches),(CS*NS,11,11,30,1)),dtype=tf.float32)\n",
    "    return tquery_patches, tsupport_patches, query_labels, support_labels, support_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_acc = 0 \n",
    "total_open_oa = 0\n",
    "total_outlier_acc = 0 \n",
    "rec_k = 0\n",
    "rec_u = 0\n",
    "f1score = 0 \n",
    "total_auc = 0\n",
    "emb_dim = 64\n",
    "tepochs = 100\n",
    "K = 5\n",
    "N = 15\n",
    "\n",
    "for i in range(tepochs) :\n",
    "    tquery_patches, tsupport_patches, query_labels, support_labels, support_classes = test_episode(patches_class_ip,5,15,3,6)   \n",
    "    accuracy, open_oa, outlier_det_acc, auc = meta_test(tsupport_patches,tquery_patches,support_labels,query_labels,support_classes,5,3,6,15)\n",
    "    total_acc = total_acc + accuracy\n",
    "    total_open_oa = total_open_oa + open_oa\n",
    "    total_outlier_acc = total_outlier_acc + outlier_det_acc\n",
    "    total_auc = total_auc + auc\n",
    "print('accuracy',total_acc*100/tepochs)\n",
    "print('Outlier detection accuracy', (total_outlier_acc*100/tepochs))\n",
    "print('open oa',total_open_oa*100/tepochs)\n",
    "print('auc',total_auc/tepochs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
